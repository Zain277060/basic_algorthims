{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba725b15",
   "metadata": {},
   "source": [
    "# KNN IMPLEMENTATION USING NUMPY EXCLUSIVELY\n",
    "K- Nearest Neighbor algorthim is supervised learning scheme which uses neighboring points of training dataset to predict testing points. It can be applied to both regression and classification problems. \n",
    "\n",
    "## K Nearest Neighbor:\n",
    "Like name suggest K Nearest Neighbors are the closest points to test point. Number of these points can be change to optimize the classification of test point. Neighbors are defined based on the distance proximity. distance from each point can be calculated using algorthims such as Euclidean Distance, Manhattan Distance formulas etc.\n",
    "Implementation of this algorthim can be done in 3 step process.\n",
    "\n",
    "### 1. DISTANCE CALCULATION\n",
    "Calculate distances for each test point against all training points. We will be using Euclidean distancing method to calculate distances.\n",
    "\n",
    "### 2. SORTING \n",
    "Select the top K number of closest points (for example if have selected k = 4, than this means that we have selected top 4 closest point to test points or 4 neighbors). This can be done simply by sorting distances from lowest to highest and selecting the top K values. In this step we will collect closest distance and original indices of corresponding points.\n",
    "\n",
    "### 3. PREDICTING\n",
    "#### Classification:\n",
    "There are basically 2 method for classifying test points, one is uniform method in which we considers neighbors classes. For instance, we have selected k=3, that mean there are 3 closes neighbors to test point, if 2 of them belong to class A while other one belongs to class B then test point will be classified as from class A. According to that method, we classify the test points regardless of the proximity or distance.Other method is inverse distancing weighting proporting, classify the points based on proximity of neighboring points from test point. Algorthim calculates the probability of each class for each test point.\n",
    "#### Regression: \n",
    "Simple KNN regression can be solved by taking mean of elements of neigboring classes. It can also be done using inverse distancing. Inverse distancing is consider more robust but it can be susceptible to amiguity when testing and training dataset contain some same features. In that case, closest distance between test and training set will be equal to zero and inverse distance will be infinity which is undesirable for analysis. To counter that issue, we can use mean method.\n",
    "\n",
    "Let s' Begin.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53956f9",
   "metadata": {},
   "source": [
    "## Importing numpy libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30f31fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def181b5",
   "metadata": {},
   "source": [
    "### Import Classification and Regression Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcc3971c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# local Path here\n",
    "path=r'D:\\Directory/'\n",
    "\n",
    "# Classification file\n",
    "class_file_name = \"classification.csv\"\n",
    "\n",
    "# Regression file\n",
    "regr_file_name  = \"regression.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71de4719",
   "metadata": {},
   "source": [
    "## Part 1 - KNN classification\n",
    "\n",
    "This algorthim will make use of inverse distancing weighting proporation to calculate the class probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "757f364c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_classify(test, train, k):\n",
    "    \n",
    "    dis=[]\n",
    "    for tr_sample in train:\n",
    "        # Calculating Euclidean distance of test data corresponding to each training point\n",
    "        dis.append(np.sqrt(np.sum( (test - tr_sample) ** 2 )))\n",
    "        \n",
    "    # Sorting for minimum distances and their indices\n",
    "    ndis=np.sort(dis)[:k]\n",
    "    nind=np.array(dis).argsort()[:k]\n",
    "    \n",
    "    # Determining labels of minimum distances neighbors\n",
    "    row_pred = train_y[nind]\n",
    "    \n",
    "    # total classes in labelled data\n",
    "    label_classes=np.unique(train_y)\n",
    "    \n",
    "    # Using inverse distancing wieghting proporation\n",
    "    inv=1/np.array(ndis)\n",
    "    # average inverse distance\n",
    "    m_inv = inv / np.sum(inv)[np.newaxis]\n",
    "    weighted_vote_count=[]\n",
    "    \n",
    "    # Loop for determining the proportional weight of each class.\n",
    "    for label in label_classes:\n",
    "        index=row_pred==label\n",
    "        weighted_vote = np.sum(m_inv[index])\n",
    "        weighted_vote_count.append(weighted_vote)\n",
    "    probable_class_index = np.argmax(weighted_vote_count)\n",
    "    label=label_classes[probable_class_index]\n",
    "    \n",
    "    return label\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b8f32f",
   "metadata": {},
   "source": [
    "### 1.2 Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aacb55fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage Correct Prediction of Validation Data = 85.0 %\n",
      "Percentage Correct Prediction of Test Data = 87.5 %\n",
      "Best K Nearest Neighbor value (k) = 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Uploading data from local machine\n",
    "data=np.genfromtxt(path+class_file_name, delimiter=',')[1:]\n",
    "\n",
    "\n",
    "#Splitting dataset into training, validating and testing as according to implied ratios.\n",
    "tr_split=int(0.6*len(data))\n",
    "val_split=int(0.8*len(data))\n",
    "\n",
    "#Training dataset\n",
    "train=data[:tr_split]\n",
    "train_x=train[:,1:]\n",
    "train_y=train[:,0].astype(int)\n",
    "\n",
    "#Validation dataset\n",
    "val=data[tr_split:val_split]\n",
    "val_x=val[:,1:]\n",
    "val_y=val[:,0].astype(int)\n",
    "\n",
    "#Testing dataset\n",
    "test=data[val_split:]\n",
    "test_x=test[:,1:]\n",
    "test_y=test[:,0].astype(int)\n",
    "\n",
    "\n",
    "\n",
    "# KNN Classification Implementation\n",
    "validation_result=[]\n",
    "\n",
    "#Looping K value\n",
    "for k in range(2,40):\n",
    "    val_predict=[]\n",
    "    \n",
    "    #Looping each feature of validation data\n",
    "    for v in val_x:\n",
    "        val_predict.append(knn_classify(v,train_x,k))\n",
    "    \n",
    "    # Calculating Percent Correct Predictions for Validation\n",
    "    val_pc=(np.count_nonzero(val_predict-val_y == 0)/np.array(val_predict).shape[0])*100\n",
    "    \n",
    "    #Appending Percent Correct Predictions value corresponding to each \"K\" value.\n",
    "    validation_result.append([val_pc,k])\n",
    "    \n",
    "    # Sorting Maximum Percent Correct Predictions along with its \"K\" value\n",
    "    validation_result.sort(key=lambda x: x[0],reverse=True)\n",
    "    \n",
    "    # Best Percent Correct Predictions and \"K\" value\n",
    "    best_validation_result = validation_result[0][0]\n",
    "    best_k = validation_result[0][1]\n",
    "\n",
    "# Predicting against test data using best \"K\" value   \n",
    "test_predict=[]   \n",
    "for t in test_x:\n",
    "    test_predict.append(knn_classify(t,train_x,best_k))\n",
    "\n",
    "# Calculating Percent Correct Predictions for Test\n",
    "test_pc=(np.count_nonzero(test_predict-test_y == 0)/np.array(test_predict).shape[0])*100\n",
    "\n",
    "\n",
    "#Results\n",
    "print(f'Percentage Correct Prediction of Validation Data = {np.round(best_validation_result,2)} %')\n",
    "print(f'Percentage Correct Prediction of Test Data = {np.round(test_pc,2)} %')\n",
    "print(f'Best K Nearest Neighbor value (k) = {np.round(best_k,1)}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625323ea",
   "metadata": {},
   "source": [
    "## Part 2 - KNN Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb888c12",
   "metadata": {},
   "source": [
    "### 2.1 KNN regression algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "403e46e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_regression(test, train, k):\n",
    "    \n",
    "    dis=[]\n",
    "    # Calculating distancing corresponding to training data\n",
    "    for tr_sample in train:\n",
    "        dis.append(np.sqrt(np.sum( (test - tr_sample) ** 2 )))\n",
    "    \n",
    "    # Sorting the minimum distance and their indices.\n",
    "    ndis=np.sort(dis)[:k]\n",
    "    nind=np.array(dis).argsort()[:k]\n",
    "    \n",
    "    # When minimum distance is zero inverse distancing yeild infinity \n",
    "    # So in that case, we consider uses mean to calculate label.\n",
    "    if ndis[0]==0.0:\n",
    "        label=train_y[nind].mean()\n",
    "        return round(label,2)\n",
    "    # Otherwise, We are using inverse distancing method\n",
    "    else:\n",
    "        row_pred = train_y[nind]\n",
    "        inv=1/np.array(ndis)\n",
    "        label = round(np.matmul(inv, row_pred) / np.sum(inv),2)\n",
    "        return label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b5feff",
   "metadata": {},
   "source": [
    "### 2.2 Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db478d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---K Nearest Neighbour Regression:\n",
      "Residual Sum of Square (RSS) of Validation Data = 0.04 \n",
      "Residual Sum of Square (RSS) of Test Data = 0.05 \n",
      "Best K Nearest Neighbor value (k) = 3\n",
      "\n",
      "---Linear Regression:\n",
      "Residual Sum of Square (RSS) of Test Data = 0.1 \n"
     ]
    }
   ],
   "source": [
    "# Uploading data from local machine\n",
    "regr_data=np.genfromtxt(path+regr_file_name, delimiter=',')[1:]\n",
    "\n",
    "#Splitting dataset into training, validating and testing as according to implied ratios.\n",
    "# Splits\n",
    "tr_split=int(0.6*len(regr_data))\n",
    "val_split=int(0.8*len(regr_data))\n",
    "\n",
    "#Training dataset\n",
    "train=regr_data[:tr_split]\n",
    "train_x=train[:,1:]\n",
    "train_y=train[:,0]\n",
    "\n",
    "#Validation dataset\n",
    "val=regr_data[tr_split:val_split]\n",
    "val_x=val[:,1:]\n",
    "val_y=val[:,0]\n",
    "\n",
    "#Testing dataset\n",
    "test=regr_data[val_split:]\n",
    "test_x=test[:,1:]\n",
    "test_y=test[:,0]\n",
    "\n",
    "\n",
    "\n",
    "#KNN Regression Implementation\n",
    "validation_result=[]\n",
    "\n",
    "# looping \"K\" value \n",
    "for k in range(2,15):\n",
    "    val_predict=[]\n",
    "    \n",
    "    #Looping each feature of validation data\n",
    "    for v in val_x:\n",
    "        val_predict.append(knn_regression(v,train_x,k))\n",
    "    \n",
    "    # Calculating Residual Sum of Square (RSS) for validation\n",
    "    val_rss=np.sum(np.square(val_predict-val_y))\n",
    "    \n",
    "    # Appending RSS value corresponding to each \"K\" value.\n",
    "    validation_result.append([val_rss,k])\n",
    "    \n",
    "    # Sorting for minimum RSS value,to get best \"K\" value\n",
    "    validation_result.sort(key=lambda x: x[0])\n",
    "    \n",
    "    # Best RSS and K value\n",
    "    best_validation_result = validation_result[0][0]\n",
    "    best_k = validation_result[0][1]\n",
    "\n",
    "# Predicting against test data using best \"K\" value\n",
    "test_predict=[]   \n",
    "for t in test_x:\n",
    "    test_predict.append(knn_regression(t,train_x,best_k))\n",
    "    \n",
    "# Calculating Residual Sum of Square (RSS) for Test\n",
    "test_rss=np.sum(np.square(test_predict-test_y))\n",
    "\n",
    "\n",
    "#Results\n",
    "print('---K Nearest Neighbour Regression:')\n",
    "print(f'Residual Sum of Square (RSS) of Validation Data = {np.round(best_validation_result,2)} ')\n",
    "print(f'Residual Sum of Square (RSS) of Test Data = {np.round(test_rss,2)} ')\n",
    "print(f'Best K Nearest Neighbor value (k) = {np.round(best_k,1)}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
